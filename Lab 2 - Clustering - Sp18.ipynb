{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Clustering ##\n",
    "INFO 254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171639, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>q8</th>\n",
       "      <th>q9</th>\n",
       "      <th>q10</th>\n",
       "      <th>q11</th>\n",
       "      <th>q12</th>\n",
       "      <th>...</th>\n",
       "      <th>q16r</th>\n",
       "      <th>q16u</th>\n",
       "      <th>q16v</th>\n",
       "      <th>q16w</th>\n",
       "      <th>q16x</th>\n",
       "      <th>q16y</th>\n",
       "      <th>q16z</th>\n",
       "      <th>q16aa</th>\n",
       "      <th>q16ab</th>\n",
       "      <th>q16ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "      <td>24205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.673497</td>\n",
       "      <td>5.532121</td>\n",
       "      <td>4.990002</td>\n",
       "      <td>9.002644</td>\n",
       "      <td>1.507177</td>\n",
       "      <td>0.930915</td>\n",
       "      <td>0.863740</td>\n",
       "      <td>1.540158</td>\n",
       "      <td>26.765398</td>\n",
       "      <td>25.708965</td>\n",
       "      <td>...</td>\n",
       "      <td>3.633588</td>\n",
       "      <td>0.460916</td>\n",
       "      <td>22.550671</td>\n",
       "      <td>25.064697</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>73.981978</td>\n",
       "      <td>0.642677</td>\n",
       "      <td>0.561124</td>\n",
       "      <td>1.141080</td>\n",
       "      <td>3.649456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.468561</td>\n",
       "      <td>25.407165</td>\n",
       "      <td>22.812399</td>\n",
       "      <td>28.164931</td>\n",
       "      <td>0.889274</td>\n",
       "      <td>0.980001</td>\n",
       "      <td>0.951531</td>\n",
       "      <td>1.027086</td>\n",
       "      <td>10.166944</td>\n",
       "      <td>11.569436</td>\n",
       "      <td>...</td>\n",
       "      <td>1.490942</td>\n",
       "      <td>0.509305</td>\n",
       "      <td>14.454279</td>\n",
       "      <td>28.650883</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>51.060694</td>\n",
       "      <td>1.427017</td>\n",
       "      <td>1.908011</td>\n",
       "      <td>3.234828</td>\n",
       "      <td>0.990191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>16.670000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>1.610000</td>\n",
       "      <td>25.930000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>61.842593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>33.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>95.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>360.000000</td>\n",
       "      <td>2333.000000</td>\n",
       "      <td>2093.000000</td>\n",
       "      <td>2420.000000</td>\n",
       "      <td>5.890000</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>7.790000</td>\n",
       "      <td>84.620000</td>\n",
       "      <td>89.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1289.000000</td>\n",
       "      <td>0.150618</td>\n",
       "      <td>549.333333</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>342.300000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 q3            q4            q5            q6            q7  \\\n",
       "count  24205.000000  24205.000000  24205.000000  24205.000000  24205.000000   \n",
       "mean       6.673497      5.532121      4.990002      9.002644      1.507177   \n",
       "std        7.468561     25.407165     22.812399     28.164931      0.889274   \n",
       "min        1.000000      1.000000      1.000000      1.000000      0.000000   \n",
       "25%        2.000000      1.000000      1.000000      2.000000      0.690000   \n",
       "50%        5.000000      2.000000      2.000000      5.000000      1.610000   \n",
       "75%        8.000000      4.000000      4.000000      9.000000      2.080000   \n",
       "max      360.000000   2333.000000   2093.000000   2420.000000      5.890000   \n",
       "\n",
       "                 q8            q9           q10           q11           q12  \\\n",
       "count  24205.000000  24205.000000  24205.000000  24205.000000  24205.000000   \n",
       "mean       0.930915      0.863740      1.540158     26.765398     25.708965   \n",
       "std        0.980001      0.951531      1.027086     10.166944     11.569436   \n",
       "min        0.000000      0.000000      0.000000      2.780000      1.410000   \n",
       "25%        0.000000      0.000000      0.690000     20.000000     16.670000   \n",
       "50%        0.690000      0.690000      1.610000     25.930000     25.000000   \n",
       "75%        1.390000      1.390000      2.200000     33.330000     33.330000   \n",
       "max        7.750000      7.650000      7.790000     84.620000     89.290000   \n",
       "\n",
       "           ...               q16r          q16u          q16v          q16w  \\\n",
       "count      ...       24205.000000  24205.000000  24205.000000  24205.000000   \n",
       "mean       ...           3.633588      0.460916     22.550671     25.064697   \n",
       "std        ...           1.490942      0.509305     14.454279     28.650883   \n",
       "min        ...           1.000000      0.000000      1.000000      1.000000   \n",
       "25%        ...           3.000000      0.000000     10.000000      9.000000   \n",
       "50%        ...           4.000000      0.333333     21.000000     17.000000   \n",
       "75%        ...           5.000000      0.666667     34.000000     32.000000   \n",
       "max        ...           5.000000      7.000000     53.000000   1289.000000   \n",
       "\n",
       "               q16x          q16y          q16z         q16aa         q16ab  \\\n",
       "count  24205.000000  24205.000000  24205.000000  24205.000000  24205.000000   \n",
       "mean       0.003781     73.981978      0.642677      0.561124      1.141080   \n",
       "std        0.006020     51.060694      1.427017      1.908011      3.234828   \n",
       "min        0.000000      0.250000      0.000000      0.000000      0.000000   \n",
       "25%        0.000477     39.125000      0.000000      0.000000      0.000000   \n",
       "50%        0.001944     61.842593      0.000000      0.000000      0.500000   \n",
       "75%        0.004666     95.375000      1.000000      0.000000      1.333333   \n",
       "max        0.150618    549.333333     44.000000    106.000000    342.300000   \n",
       "\n",
       "              q16ac  \n",
       "count  24205.000000  \n",
       "mean       3.649456  \n",
       "std        0.990191  \n",
       "min        1.000000  \n",
       "25%        3.200000  \n",
       "50%        3.800000  \n",
       "75%        4.333333  \n",
       "max        5.000000  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('yelp_reviewers.csv')\n",
    "print(df.shape)\n",
    "df.dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 ###\n",
    "\n",
    "Choose an implementation of k-means and specify it in the google sheet  \n",
    "\n",
    "** Answer **:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans # Scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ###\n",
    "What is the best choice of k according to the silhouette metric for clustering q4-q6 (# of cool, funny, useful votes). Only consider 2 <= k <= 8.  \n",
    "\n",
    "NOTE: For features with high variance, empty clusters can occur. There are several ways of dealing with empty clusters. A common approach is to drop empty clusters, the prefered approach for this Lab is to treat the empty cluster as a “singleton” leaving it empty with a single point placeholder. This can be accomplished in MATLAB, for example, with the command:  [IDX,C]=kmeans(d,4, 'Distance', 'sqeuclidean','EmptyAction','singleton')\n",
    "\n",
    "**Answer**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice: k =  2 , silhouette score =  0.98939672069\n"
     ]
    }
   ],
   "source": [
    "current_max, current_index = float(\"-inf\"),-1\n",
    "#new_df = df.dropna().sample(n = 10000)   # Sampling 10000 data points randomly since the whole dataset is too large to load\n",
    "#print(type(new_df))  # Dataframe\n",
    "#b = [1,3,5]\n",
    "#print(df.iloc[:,b])\n",
    "\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(df.iloc[:, 1:4])\n",
    "    s_score = silhouette_score(df.iloc[:,1:4], kmeans.labels_, sample_size = 10000)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "\n",
    "print(\"Best choice: k = \", current_index, \", silhouette score = \", current_max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 3 ###\n",
    "Answer question 2 but using the log of the features (q7-q10)\n",
    "\n",
    "**Answer**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom sklearn.preprocessing import Imputer\\n\\ncol_loc = new_df.columns.get_loc(\\'q7\\')\\n#new_df.iloc[:,col_loc:(col_loc+4)].head()\\n\\nlog_new_df = np.log(new_df.iloc[:,col_loc:(col_loc+4)])\\nlog_new_df[log_new_df == float(\\'-inf\\')] = float(\\'nan\\')\\n#print(log_new_df.head())\\nimputer = Imputer()\\nlog_new_df = imputer.fit_transform(log_new_df) # It is an array now\\n#print(log_new_df[:5])\\n\\n\\n\\ncurrent_max, current_index = float(\"-inf\"),-1\\nfor k in range(2,9):\\n    kmeans = KMeans(n_clusters = k).fit(log_new_df)\\n    s_score = silhouette_score(log_new_df, kmeans.labels_)\\n    if s_score > current_max:\\n        current_max = s_score\\n        current_index = k\\nprint(\"Best choice: k = \", current_index)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I had taken log of the features q7-q10 in this code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "col_loc = new_df.columns.get_loc('q7')\n",
    "#new_df.iloc[:,col_loc:(col_loc+4)].head()\n",
    "\n",
    "log_new_df = np.log(new_df.iloc[:,col_loc:(col_loc+4)])\n",
    "log_new_df[log_new_df == float('-inf')] = float('nan')\n",
    "#print(log_new_df.head())\n",
    "imputer = Imputer()\n",
    "log_new_df = imputer.fit_transform(log_new_df) # It is an array now\n",
    "#print(log_new_df[:5])\n",
    "\n",
    "\n",
    "\n",
    "current_max, current_index = float(\"-inf\"),-1\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(log_new_df)\n",
    "    s_score = silhouette_score(log_new_df, kmeans.labels_)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "print(\"Best choice: k = \", current_index)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice: k =  2 , silhouette score =  0.525805639106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "col_loc = df.columns.get_loc('q7')\n",
    "\n",
    "\n",
    "log_df = df.iloc[:,col_loc:(col_loc+4)]\n",
    "log_df[log_df == float('-inf')] = float('nan')\n",
    "#print(log_new_df.head())\n",
    "imputer = Imputer()\n",
    "log_df = imputer.fit_transform(log_df) # It is an array now\n",
    "#print(log_new_df[:5])\n",
    "\n",
    "\n",
    "\n",
    "current_max, current_index = float(\"-inf\"),-1\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(log_df)\n",
    "    s_score = silhouette_score(log_df, kmeans.labels_, sample_size = 10000)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "print(\"Best choice: k = \", current_index, \", silhouette score = \", current_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_df.iloc[:,col_loc:(col_loc+4)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 4 ###\n",
    "Answer question 2 but using the percentage of the features (q11-q13)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "Best choice: k =  8\n"
     ]
    }
   ],
   "source": [
    "# Here again, I took the percentage of each column w.r.t the sum of all the 3 columns\n",
    "\n",
    "\"\"\"\n",
    "col_loc = new_df.columns.get_loc('q11')\n",
    "new_df_slice = new_df.iloc[:,col_loc:(col_loc+3)]\n",
    "sum_across_cols = new_df_slice.sum(axis=1) # Axis of repeat isn't supported in pandas' implememtation of repeat\n",
    "\n",
    "\n",
    "#print(type(sum_across_cols))   # Series\n",
    "#print(np.repeat(sum_across_cols, 3, axis =1).shape)\n",
    "#percent_new_df = np.divide(, np.repeat(sum_across_cols, 3, axis=1))*100\n",
    "\n",
    "for i in range(len(new_df_slice.iloc[0])):      # new_df_slice[:1] wasn't giving the desired results, think why\n",
    "    new_df_slice.iloc[:,i] = np.divide(new_df_slice.iloc[:,i], sum_across_cols)*100\n",
    "#new_df_slice.describe()\n",
    "\n",
    "current_max, current_index = float(\"-inf\"),-1\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(new_df_slice)\n",
    "    s_score = silhouette_score(new_df_slice, kmeans.labels_)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "print(\"Best choice: k = \", current_index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice: k =  8\n"
     ]
    }
   ],
   "source": [
    "col_loc = df.columns.get_loc('q11')\n",
    "df_slice = df.iloc[:,col_loc:(col_loc+3)]\n",
    "\n",
    "imputer = Imputer()\n",
    "df_slice = imputer.fit_transform(df_slice)\n",
    "current_max, current_index = float(\"-inf\"),-1\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(df_slice)\n",
    "    s_score = silhouette_score(df_slice, kmeans.labels_, sample_size = 10000)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "print(\"Best choice: k = \", current_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 5 ###\n",
    "Inspect the [best] clustering generated from question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a ####\n",
    "**Question**: List the number of data points in each cluster (eg. C1: 2,000 C2: 4,200 etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0 : 8677, C1 : 14865, C2 : 86641, C3 : 5805, C4 : 4026, C5 : 10188, C6 : 32922, C7 : 8515, "
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = current_index).fit(df_slice)\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for label in kmeans.labels_:\n",
    "    cnt[label]+=1\n",
    "for item in sorted(cnt.items(), key = lambda x: x[0]): # item is (key,value)\n",
    "    print(\"C\"+str(item[0]),\":\",item[1], end = \", \")\n",
    "#print(sorted(cnt, key = lambda x:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b ####\n",
    "**Question**: Were there clusters that represented very funny but useless reviewers?  \n",
    "\n",
    "**Answer**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26.35629771   1.56193359  72.08165802]\n",
      " [ 32.79166095  33.46908443  33.73584191]\n",
      " [ 19.21344078  18.71325288  62.07314944]\n",
      " [  1.36427907  97.08347976   1.55237037]\n",
      " [ 98.20565574   0.89977894   0.894615  ]\n",
      " [ 49.12774441   2.46021692  48.41214861]\n",
      " [  0.17617186   0.40948758  99.41432112]\n",
      " [  2.01080446  46.42119789  51.56799883]]\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the coordinates of the cluster centers, we see that cluster 3 (in this case, the exact label could vary depending on initializations etc.) has only about 1.5% from useful votes and about 97% from funny votes. Instead of just looking at the cluster centers and saying this (which would be hard if there were many centers, we could also have a threshold, say 80%, for the difference between the percentages of funny and useful votes and return all clusters for which the threshold is exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.c ####\n",
    "**Question**: How many reviewers were in the cluster that represented relatively equal strength in all voting categories (assuming such a cluster exists in your clustering)?  \n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of reviewers in the cluster that represented relatively equal strength in all voting categories:  14865\n"
     ]
    }
   ],
   "source": [
    "latest_cluster = -1 # The latest cluster that satisfies the required criteria\n",
    "for i,centers in enumerate(kmeans.cluster_centers_):\n",
    "    if max(centers)-min(centers)<10:  # some heuristic threshold\n",
    "        latest_cluster = i\n",
    "if latest_cluster == -1:\n",
    "    print(\"No such cluster found\")\n",
    "else:\n",
    "    print(\"The number of reviewers in the cluster that represented relatively equal strength in all voting categories: \",cnt[latest_cluster])\n",
    "        \n",
    "# We could also have eyeballed it in this case since we see that cluster 6 has almost equal coordinates, but my code is more general, ofcourse with the caveat of having to select the threshold       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 6 ###\n",
    "Cluster the dataset using $k = 5$ and using features q7-q13 (log and % type votes) and q14 (most active year feature) and the natural log of q15 (avg review chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_loc = df.columns.get_loc('q7')\n",
    "q6_df = df.iloc[:,col_loc:(col_loc+8)]   #q6 refers to question 6 here, not some feature\n",
    "#new_df.iloc[:,new_df.columns.get_loc('q15')]\n",
    "q6_df = q6_df.join(pd.DataFrame(np.log(df.iloc[:,df.columns.get_loc('q15')]))) \n",
    "imputer = Imputer()\n",
    "q6_df = imputer.fit_transform(q6_df)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5).fit(q6_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.a ####\n",
    "**Question**: What is the silhouette metric for this clustering?   \n",
    "You may use the max, as you did in question 2. For a more in-depth understanding of cluster analysis with silhouette, look [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The silhouette metric for this clustering is:  0.648573840098\n"
     ]
    }
   ],
   "source": [
    "s_score = silhouette_score(q6_df, kmeans.labels_, sample_size = 10000)\n",
    "print(\"The silhouette metric for this clustering is: \", s_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.b ####\n",
    "**Question**: What was the average “number of reviews per reviewer (q3)” among the points in each of the clusters (eg. C1: 1.4 C2: 4.2 C3: 3.4 etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0 : 5.13274199769, C1 : 2.87911339962, C2 : 2.67239955971, C3 : 2.85209182152, C4 : 2.60288629012, "
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "#for i in range(5):\n",
    "#    dict1[i] = list()\n",
    "#print(kmeans.labels_[9999])\n",
    "#print([new_df.iloc[9999,new_df.columns.get_loc('q3')]])\n",
    "#print()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if kmeans.labels_[i] in dict1:\n",
    "        dict1[kmeans.labels_[i]].append(df.iloc[i,df.columns.get_loc('q3')])\n",
    "    else:\n",
    "        dict1[kmeans.labels_[i]] = ([df.iloc[i,df.columns.get_loc('q3')]])\n",
    "#print(dict1.keys())\n",
    "for key in sorted(dict1.keys()):\n",
    "    print(\"C\"+str(key),\":\",np.mean(np.array(dict1[key])), end =\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Question 7 ###\n",
    "Cluster the dataset using the features described in question 6 + every group’s question 16 features (you may drop features with high incidents of -Inf / blank / or NaN values). It is suggested that you perform some form of normalization on these question 16 features so as not to over bias the clustering towards the larger magnitude features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleansing and Normalization ####\n",
    "Check how many null values there are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q7': 0, 'q8': 117853, 'q9': 122162, 'q10': 81153, 'q11': 71282, 'q12': 71282, 'q13': 71282, 'q14': 0, 'q15': 0, 'q16a': 0, 'q16b': 0, 'q16c': 0, 'q16d': 0, 'q16e': 0, 'q16f': 0, 'q16g': 0, 'q16h': 0, 'q16i': 0, 'q16j': 0, 'q16k': 0, 'q16l': 0, 'q16m': 0, 'q16n': 0, 'q16o': 0, 'q16p': 0, 'q16q': 0, 'q16r': 0, 'q16s': 0, 'q16t': 0, 'q16u': 0, 'q16v': 0, 'q16w': 0, 'q16x': 0, 'q16y': 0, 'q16z': 0, 'q16aa': 0, 'q16ab': 47594, 'q16ac': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "col_loc = df.columns.get_loc('q7')\n",
    "q6_df = df.iloc[:,col_loc:(col_loc+8)]   #q6 refers to question 6 here, not some feature\n",
    "q6_df = q6_df.join(pd.DataFrame(np.log(df.iloc[:,df.columns.get_loc('q15')])))\n",
    "q7_df = pd.DataFrame(q6_df).join(df.iloc[:,df.columns.get_loc('q16a'):])\n",
    "#print(q7_df.shape)\n",
    "#print(q7_df.head())\n",
    "#print(q7_df['q8'].isnull().sum())\n",
    "\n",
    "d = {}\n",
    "for col in q7_df.columns:\n",
    "    d[col] = np.sum(q7_df[col].isnull().sum())\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like q8 - q13 and q16ab have a lot of null values, especially q8 and q9. Let's see what the impact is of removing some of these columnsbefore removing any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(q7_df.columns)\n",
    "del q7_df['q8']   # Could also do df.drop\n",
    "del q7_df['q9']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing 2 features, we double the number of rows remaining. That's pretty good.  \n",
    "Preprocess categorical variables to dummy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{dtype('float64'), dtype('int64'), dtype('O')}\n",
      "     q7   q10   q11   q12    q13  q14       q15  q16a      q16b      q16c  \\\n",
      "1  1.10  0.00   0.0   0.0  100.0    9  5.030438     0  0.577350  0.002179   \n",
      "3  1.10  0.00   0.0   0.0  100.0   10  6.219934     1  2.309401  0.000663   \n",
      "5  1.10  1.10   0.0   0.0  100.0    9  5.950643     0  2.309401  0.002604   \n",
      "6  2.08  1.79  25.0   0.0   75.0    9  6.013104     5  1.995531  0.002446   \n",
      "9  0.69  0.00   0.0  50.0   50.0    9  5.898527     1  2.121320  0.002743   \n",
      "\n",
      "     ...     q16t  q16u  q16v  q16w      q16x       q16y  q16z  q16aa  \\\n",
      "1    ...        1   1.0    19    14  0.000000  21.833333     1      0   \n",
      "3    ...        1   0.0     8    11  0.002242  61.500000     0      0   \n",
      "5    ...        1   0.0    27    11  0.005814  41.000000     0      0   \n",
      "6    ...        0   0.5     1    27  0.006479  42.156250     0      0   \n",
      "9    ...        1   0.5    31     7  0.000000  56.750000     0      0   \n",
      "\n",
      "      q16ab     q16ac  \n",
      "1  0.333333  4.666667  \n",
      "3  0.333333  3.666667  \n",
      "5  1.000000  3.666667  \n",
      "6  1.000000  3.375000  \n",
      "9  1.000000  3.500000  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "q7_df = q7_df.dropna() # Till now I have been imputing and not dropping the NaN values, but siunce this question refers to dropping NaN, I have done so here\n",
    "list_of_types = []\n",
    "for col in q7_df.columns:\n",
    "    list_of_types.append(q7_df[col].dtype)\n",
    "print(set(list_of_types))\n",
    "\n",
    "for col in q7_df.columns:\n",
    "    if q7_df[col].dtype == 'O':\n",
    "        cats = set(q7_df[col])\n",
    "        #print(cats)\n",
    "        count = 0\n",
    "        d = {}\n",
    "        for elt in cats:\n",
    "            d[elt] = count\n",
    "            count+=1\n",
    "        #print(d)\n",
    "        #break\n",
    "        new_q7 = []\n",
    "        for elt in q7_df[col]:\n",
    "            new_q7.append(d[elt])\n",
    "        q7_df[col]=new_q7\n",
    "            \n",
    "print(q7_df.head())           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q7</th>\n",
       "      <th>q10</th>\n",
       "      <th>q11</th>\n",
       "      <th>q12</th>\n",
       "      <th>q13</th>\n",
       "      <th>q14</th>\n",
       "      <th>q15</th>\n",
       "      <th>q16a</th>\n",
       "      <th>q16b</th>\n",
       "      <th>q16c</th>\n",
       "      <th>...</th>\n",
       "      <th>q16t</th>\n",
       "      <th>q16u</th>\n",
       "      <th>q16v</th>\n",
       "      <th>q16w</th>\n",
       "      <th>q16x</th>\n",
       "      <th>q16y</th>\n",
       "      <th>q16z</th>\n",
       "      <th>q16aa</th>\n",
       "      <th>q16ab</th>\n",
       "      <th>q16ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.511036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.677844</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.005809</td>\n",
       "      <td>0.089547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186757</td>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.640080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.059576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.353141</td>\n",
       "      <td>0.229782</td>\n",
       "      <td>0.267867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.738083</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.648840</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.705527</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020186</td>\n",
       "      <td>0.016786</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559973</td>\n",
       "      <td>0.476166</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.632772</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.004658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         q7       q10       q11       q12       q13       q14       q15  q16a  \\\n",
       "0  0.186757  0.000000  0.000000  0.000000  1.000000  0.833333  0.511036   0.0   \n",
       "1  0.186757  0.000000  0.000000  0.000000  1.000000  0.916667  0.677844   0.1   \n",
       "2  0.186757  0.141207  0.000000  0.000000  1.000000  0.833333  0.640080   0.0   \n",
       "3  0.353141  0.229782  0.267867  0.000000  0.738083  0.833333  0.648840   0.5   \n",
       "4  0.117148  0.000000  0.000000  0.559973  0.476166  0.833333  0.632772   0.1   \n",
       "\n",
       "       q16b      q16c    ...     q16t      q16u      q16v      q16w      q16x  \\\n",
       "0  0.204124  0.000654    ...      1.0  0.111111  0.346154  0.010093  0.000000   \n",
       "1  0.816497  0.000187    ...      1.0  0.000000  0.134615  0.007764  0.005809   \n",
       "2  0.816497  0.000784    ...      1.0  0.000000  0.500000  0.007764  0.015063   \n",
       "3  0.705527  0.000736    ...      0.0  0.055556  0.000000  0.020186  0.016786   \n",
       "4  0.750000  0.000827    ...      1.0  0.055556  0.576923  0.004658  0.000000   \n",
       "\n",
       "       q16y      q16z  q16aa     q16ab     q16ac  \n",
       "0  0.031555  0.022727    0.0  0.000974  0.916667  \n",
       "1  0.089547  0.000000    0.0  0.000974  0.666667  \n",
       "2  0.059576  0.000000    0.0  0.002921  0.666667  \n",
       "3  0.061266  0.000000    0.0  0.002921  0.593750  \n",
       "4  0.082602  0.000000    0.0  0.002921  0.625000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = q7_df.columns\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(q7_df)\n",
    "q7_df = pd.DataFrame(scaler.transform(q7_df), columns = cols)\n",
    "#q7_df = pd.DataFrame(normalize(q7_df, axis = 0))\n",
    "q7_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.a ####\n",
    "**Question**: Using the silhouette metric, what was the best k?  \n",
    "\n",
    "**Answer**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice: k =  2 , silhouette score =  0.232197223377\n"
     ]
    }
   ],
   "source": [
    "current_max, current_index = float(\"-inf\"),-1\n",
    "sse = []\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(n_clusters = k).fit(q7_df)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    s_score = silhouette_score(q7_df, kmeans.labels_, sample_size = 10000)\n",
    "    if s_score > current_max:\n",
    "        current_max = s_score\n",
    "        current_index = k\n",
    "\n",
    "print(\"Best choice: k = \", current_index, \", silhouette score = \", current_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.b ####\n",
    "**Question**: Using the the sum of within cluster variance metric with the elbow method what was the best k?  \n",
    "**Answer**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21b84912e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = range(2,9)\n",
    "plt.plot(k,sse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By the elbow method, it is seen that maximum slope change occurs at k=3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 ###\n",
    "For this question please come up with your own question about this dataset and using a clustering technique as part of your method of answering it. This question answer should be submitted to a bCourses assignment in the form a pdf. This report is expected to be between 2 and 3 pages. It is meant to give you practice writing up your results. The report should have the following sections:\n",
    "\n",
    "\n",
    "RESEARCH QUESTION: Describe what it is you want to ask of this dataset\n",
    "                 (the question can not be the same as question 9)\n",
    "                 \n",
    "                 \n",
    "DATASET: describe the yelp_reviews to yelp_reviewers transformation. have a subsection called FEATURES SELECTED: briefly describe the features from the dataset you choose (at least 1 features should be from a q16)\n",
    "\n",
    "\n",
    "METHODS: Briefly describe the methods you used (such as k-means) and why you used them\n",
    "\n",
    "\n",
    "RESULTS: Here you can tell the story of how you investigated the question (plots are always nice) and the conclusions you drew.\n",
    "\n",
    "\n",
    "NOTE: If you are hypothesizing that clusters might be formed with respect to a particular attribute, consider withholding that attribute from clustering and then looking to see what the mean value or distribution of that attribute is among the formed clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I have submitted the pdf and a separate Ipython notebook for this titled Lab2 Q8.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question (+15%) - Reviewer overlap:\n",
    "Create a dataset with f reviewers as the rows and business_ids as the columns (there are a lot) where the feature value is is ‘1’ if the reviewer has written a review for that business and ‘0’ if not. Use the methods described in this assignment to answer the question of how much overlap of businesses reviewed exists among reviewers in this dataset. Append this answer to your question 8 document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attached after Q8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
